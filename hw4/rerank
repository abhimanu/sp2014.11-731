#!/usr/bin/env python2
import optparse
import sys
import bleu
import codecs
#from itertools import izip
#from sklearn.linear_model import LogisticRegression

from nltk.classify.naivebayes import NaiveBayesClassifier as nb
from nltk.classify.decisiontree import DecisionTreeClassifier as dt

optparser = optparse.OptionParser()
optparser.add_option("-k", "--kbest-list", dest="dev_input", default="data/dev.100best", help="100-best translation lists")
optparser.add_option("-r", "--reference-list", dest="dev_ref_input", default="data/dev.ref")
optparser.add_option("-t", "--test-list", dest="test_input", default="data/test.100best")
(opts, _) = optparser.parse_args()


#from hw2
def getRecall(hypothesis, reference):
  full_wt = 0.5
  trunc_wt = 0.5

  hwords = hypothesis.lower().split()
  rwords = reference.lower().split()
  hypothesisset = set(hwords)
  referenceset = set(rwords)

  hwords_trunc = [word[:3] for word in hwords]
  rwords_trunc = [word[:3] for word in rwords]
  hypothesisset_trunc = set(hwords_trunc)
  referenceset_trunc = set(rwords_trunc)

  recall = (sum(full_wt for word in referenceset if word in hypothesisset) + \
              sum(trunc_wt for word in referenceset_trunc if word in hypothesisset_trunc)) / len(referenceset)

  match=0
  for i in xrange(0,len(reference)):
	  if i>=len(hypothesis):
		  break
	  if hypothesis[i]==reference[i]:
		  match+=1
	  else:
		  break
  full_match = match/len(reference)
  return recall + full_match

def getRankSort(feat1, feat2, c):
  diff = {}
  for k in feat1.keys():
    diff[k] = feat1[k] - feat2[k]
  return c.classify(diff)

def notTranslated(w):
  try:
    decoded = w.encode('ascii')
    return False
  except UnicodeDecodeError:
    return True

def getBaseFeatureVec(hypothesis):
  feature = {}
  feature['len'] = len(hypothesis.split())
  feature['num_untranslated'] = sum([1.0 for w in hypothesis.split() if notTranslated(w)])
  return feature

all_references = [sent for sent in codecs.open(opts.dev_ref_input, encoding='utf-8')]
all_hypotheses = [pair.split(' ||| ') for pair in open(opts.dev_input)]
num_sents = len(all_hypotheses) / 100
training_instances = []
sys.stderr.write("Extract features\n")
for sent in xrange(0, num_sents):
  sys.stderr.write(".")
  reference = all_references[sent]
  hypotheses_for_one_sent = all_hypotheses[sent * 100:sent * 100 + 100]
  processed_hypotheses = []
  for (num, hypothesis, feats) in hypotheses_for_one_sent:
    feat_vec = getBaseFeatureVec(hypothesis)
    for feat in feats.split(' '):
      (ky, val) = feat.split('=')
      feat_vec[ky] = float(val)

    bleu_stats = [st for st in bleu.bleu_stats(hypothesis, reference)]
    wt = 0.5
    score = wt *bleu.bleu(bleu_stats)
    score += (1.0-wt) * getRecall(hypothesis, reference)
    processed_hypotheses.append((hypothesis, score, feat_vec))

  for i in xrange(len(processed_hypotheses)):
    (hyp1, score1, feat1) = processed_hypotheses[i]
    for j in xrange(0, i):
      (hyp2, score2, feat2) = processed_hypotheses[j]
      f = {}
      for ky in feat1.keys():
        f[ky] = feat1[ky]-feat2[ky]
      hyp1_better = 1 if score1 > score2 else -1
      instance = (f, hyp1_better)
      training_instances.append(instance)

sys.stderr.write("Train Classifier\n")
c=nb.train(training_instances)
#c=dt.train(training_instances)
sys.stderr.write("Test\n")

test_hypotheses = [pair.split(' ||| ') for pair in open(opts.test_input)]
n = len(test_hypotheses)/100
for sent in xrange(0, n):
  hypotheses_for_one_sent = test_hypotheses[sent * 100:sent * 100 + 100]
  hypotheses = []
  for (num, hypothesis, feats) in hypotheses_for_one_sent:
    feat_vec = getBaseFeatureVec(hypothesis)
    for feat in feats.split(' '):
      (ky, val) = feat.split('=')
      feat_vec[ky] = float(val)
    hypotheses.append((hypothesis, feat_vec))

  best_list = sorted(hypotheses, reverse=True, cmp=(lambda (hyp1,feat1),(hyp2,feat2): getRankSort(feat1,feat2,c)))
  try:
    (best, _) = best_list[0]
    sys.stdout.write("%s\n" % best)
  except (Exception):
    sys.exit(1)
